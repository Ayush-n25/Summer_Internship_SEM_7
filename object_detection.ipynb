{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install distro\n",
    "import distro\n",
    "if \"debian\" in distro.linux_distribution()[0].lower():\n",
    "    ! apt-get update\n",
    "    ! apt-get install ffmpeg libsm6 libxext6  -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! apt-get update\n",
    "! apt-get install -y libgl1-mesa-glx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install opencv-python-headless\n",
    "!{sys.executable} -m pip install mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = \"Licenseplate_Detection\"\n",
    "\n",
    "print(\"s3://{}/{}/\".format(bucket, prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For uploading the images to s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from botocore.exceptions import NoCredentialsError, PartialCredentialsError\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "source_bucket = 'Obj_detection_SSD'\n",
    "destination_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "prefixes_to_copy = ['train/labels/', 'valid/labels/']\n",
    "\n",
    "def copy_to_sagemaker_bucket(source_bucket, destination_bucket, prefixes):\n",
    "    try:\n",
    "        for prefix in prefixes:\n",
    "            paginator = s3_client.get_paginator('list_objects_v2')\n",
    "            page_iterator = paginator.paginate(Bucket=source_bucket, Prefix=prefix)\n",
    "\n",
    "            for page in page_iterator:\n",
    "                if 'Contents' not in page:\n",
    "                    print(f\"No objects found with prefix: {prefix}\")\n",
    "                    continue\n",
    "\n",
    "                for obj in page['Contents']:\n",
    "                    source_key = obj['Key']\n",
    "                    destination_key = source_key  \n",
    "\n",
    "                    copy_source = {'Bucket': source_bucket, 'Key': source_key}\n",
    "\n",
    "                    try:\n",
    "                        #print(f\"Copying: {source_key} to s3://{destination_bucket}/{destination_key}\")\n",
    "                        s3_client.copy_object(CopySource=copy_source, Bucket=destination_bucket, Key=destination_key)\n",
    "                        #print(f\"Copy complete: {source_key}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error copying {source_key}: {e}\")\n",
    "    \n",
    "    except (NoCredentialsError, PartialCredentialsError) as e:\n",
    "        print(f\"Credentials not available: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "copy_to_sagemaker_bucket(source_bucket, destination_bucket, prefixes_to_copy)\n",
    "print(\"Specified files have been copied from source bucket to the default SageMaker S3 bucket.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the dataset in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import boto3\n",
    "s3 = boto3.client('s3')\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = \"\"\n",
    "\n",
    "local_data_dir = './data/'\n",
    "train_images_dir = os.path.join(local_data_dir, 'train/images/')\n",
    "train_labels_dir = os.path.join(local_data_dir, 'train/labels/')\n",
    "val_images_dir = os.path.join(local_data_dir, 'val/images/')\n",
    "val_labels_dir = os.path.join(local_data_dir, 'val/labels/')\n",
    "test_images_dir = os.path.join(local_data_dir, 'test/images/')\n",
    "test_labels_dir = os.path.join(local_data_dir, 'test/labels/')\n",
    "\n",
    "os.makedirs(train_images_dir, exist_ok=True)\n",
    "os.makedirs(train_labels_dir, exist_ok=True)\n",
    "os.makedirs(val_images_dir, exist_ok=True)\n",
    "os.makedirs(val_labels_dir, exist_ok=True)\n",
    "os.makedirs(test_images_dir, exist_ok=True)\n",
    "os.makedirs(test_labels_dir, exist_ok=True)\n",
    "\n",
    "def download_from_s3(prefix, local_dir):\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    for result in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "        for obj in result.get('Contents', []):\n",
    "            if not obj['Key'].endswith('/'):\n",
    "                local_file_path = os.path.join(local_dir, os.path.basename(obj['Key']))\n",
    "                s3.download_file(bucket, obj['Key'], local_file_path)\n",
    "\n",
    "download_from_s3(f'train/images/', train_images_dir)\n",
    "download_from_s3(f'train/labels/', train_labels_dir)\n",
    "download_from_s3(f'valid/images/', val_images_dir)\n",
    "download_from_s3(f'valid/labels/', val_labels_dir)\n",
    "download_from_s3(f'test/images/', test_images_dir)\n",
    "download_from_s3(f'test/labels/', test_labels_dir)\n",
    "print(\"Done copied the labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the labels for working with SSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update\n",
    "!apt-get install -y libgl1-mesa-glx\n",
    "!pip install opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def convert_yolo_to_ssd(image_path, label_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    height, width, _ = image.shape\n",
    "\n",
    "    ssd_labels = []\n",
    "    with open(label_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    for line in lines:\n",
    "        class_id, x_center, y_center, bbox_width, bbox_height = map(float, line.split())\n",
    "        xmin = int((x_center - bbox_width / 2) * width)\n",
    "        ymin = int((y_center - bbox_height / 2) * height)\n",
    "        xmax = int((x_center + bbox_width / 2) * width)\n",
    "        ymax = int((y_center + bbox_height / 2) * height)\n",
    "        ssd_labels.append([class_id, xmin, ymin, xmax, ymax])\n",
    "    \n",
    "    return ssd_labels\n",
    "\n",
    "def create_ssd_labels(images_dir, labels_dir, output_file):\n",
    "    ssd_labels = []\n",
    "    for label_file in os.listdir(labels_dir):\n",
    "        image_file = label_file.replace('.txt', '.jpg')\n",
    "        image_path = os.path.join(images_dir, image_file)\n",
    "        label_path = os.path.join(labels_dir, label_file)\n",
    "        if os.path.exists(image_path):\n",
    "            labels = convert_yolo_to_ssd(image_path, label_path)\n",
    "            for label in labels:\n",
    "                class_id, xmin, ymin, xmax, ymax = label\n",
    "                ssd_labels.append([image_file, xmin, ymin, xmax, ymax, class_id])\n",
    "    \n",
    "    df = pd.DataFrame(ssd_labels, columns=['image_file', 'xmin', 'ymin', 'xmax', 'ymax', 'class_id'])\n",
    "    df.to_csv(output_file, index=False, header=False)\n",
    "\n",
    "create_ssd_labels('./data/train/images', './data/train/labels', 'train_ssd_labels.csv')\n",
    "print(\"train done\")\n",
    "create_ssd_labels('./data/val/images', './data/val/labels', 'val_ssd_labels.csv')\n",
    "print(\"val done\")\n",
    "create_ssd_labels('./data/test/images', './data/test/labels', 'test_ssd_labels.csv')\n",
    "print(\"test csv done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create list files for generating RecordIo files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def generate_lst_file(csv_file, images_dir, output_lst_file):\n",
    "    df = pd.read_csv(csv_file, header=None)\n",
    "    with open(output_lst_file, 'w') as f:\n",
    "        for index, row in df.iterrows():\n",
    "            image_file = row[0]\n",
    "            labels = \"\\t\".join(map(str, row[1:]))\n",
    "            f.write(f\"{index}\\t{labels}\\t{image_file}\\n\")\n",
    "\n",
    "train_csv_file = \"train_ssd_labels.csv\"\n",
    "val_csv_file = \"val_ssd_labels.csv\"\n",
    "test_csv_file = \"test_ssd_labels.csv\"\n",
    "\n",
    "train_images_dir = \"./data/train/images\"\n",
    "val_images_dir = \"./data/val/images\"\n",
    "test_images_dir = \"./data/test/images\"\n",
    "\n",
    "generate_lst_file(train_csv_file, train_images_dir, \"train.lst\")\n",
    "generate_lst_file(val_csv_file, val_images_dir, \"val.lst\")\n",
    "generate_lst_file(test_csv_file, test_images_dir, \"test.lst\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate RecordIO files from list files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy==1.23.1\n",
    "!pip install mxnet\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python im2rec.py train.lst ./data/train/images --pack-label --num-thread 4\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python im2rec.py test.lst ./data/test/images --pack-label --num-thread 4\n",
    "print(\"Done for test.rec\")\n",
    "!python im2rec.py val.lst ./data/val/images --pack-label --num-thread 4\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the RecordIO files in the default s3 of sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "sagemaker_session = sagemaker.Session()\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "def upload_to_s3(local_file, s3_path, bucket):\n",
    "    s3_client.upload_file(local_file, bucket, s3_path)\n",
    "\n",
    "upload_to_s3(f'./train.rec', f'train/train.rec',default_bucket)\n",
    "print(\"train.rec uploaded to s3 of sagemaker\")\n",
    "\n",
    "upload_to_s3(f'./val.rec', f'valid/val.rec',default_bucket)\n",
    "print(\"val.rec uploaded to s3 of sagemaker\")\n",
    "\n",
    "upload_to_s3(f'./test.rec', f'test/test.rec',default_bucket)\n",
    "print(\"test.rec uploaded to s3 of sagemaker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import HyperparameterTuner, ContinuousParameter, IntegerParameter, CategoricalParameter\n",
    "import sagemaker\n",
    "from sagemaker.estimator import Estimator\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "image_uri = sagemaker.image_uris.retrieve(framework='object-detection', region=region)\n",
    "\n",
    "od_model = Estimator(\n",
    "    image_uri=image_uri,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.g4dn.2xlarge',\n",
    "    output_path=f's3://{default_bucket}/output/',\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "od_model.set_hyperparameters(\n",
    "    num_classes=1,\n",
    "    base_network='resnet-50',\n",
    "    use_pretrained_model=1,\n",
    "    lr_scheduler_step=10,\n",
    "    lr_scheduler_factor=0.1,\n",
    "    overlap_threshold=0.5,\n",
    "    nms_threshold=0.45,\n",
    "    image_shape=512,\n",
    "    label_width=5,\n",
    "    num_training_samples=887\n",
    ")\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    'learning_rate': ContinuousParameter(1e-6, 0.5),\n",
    "    'mini_batch_size': IntegerParameter(4, 16),\n",
    "    'momentum': ContinuousParameter(0.0, 0.999),\n",
    "    'optimizer': CategoricalParameter(['sgd', 'adam', 'rmsprop', 'adadelta']),\n",
    "    'weight_decay': ContinuousParameter(0.0, 0.999)\n",
    "}\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator=od_model,\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    objective_metric_name='validation:mAP',  \n",
    "    objective_type='Maximize',\n",
    "    max_jobs=7,\n",
    "    max_parallel_jobs=1\n",
    ")\n",
    "\n",
    "train_input = sagemaker.inputs.TrainingInput(\n",
    "    s3_data=f's3://{default_bucket}/train/',\n",
    "    content_type='application/x-recordio'\n",
    ")\n",
    "val_input = sagemaker.inputs.TrainingInput(\n",
    "    s3_data=f's3://{default_bucket}/valid/',\n",
    "    content_type='application/x-recordio'\n",
    ")\n",
    "\n",
    "tuner.fit({'train': train_input, 'validation': val_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "image_uri = sagemaker.image_uris.retrieve(framework='object-detection', region=region)\n",
    "\n",
    "s3_output_path = f's3://{default_bucket}/output/'\n",
    "\n",
    "od_model = Estimator(\n",
    "    image_uri=image_uri,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.g4dn.2xlarge',\n",
    "    volume_size=50,\n",
    "    max_run=360000,\n",
    "    input_mode='File',\n",
    "    output_path=s3_output_path,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "od_model.set_hyperparameters(\n",
    "    base_network='resnet-50',\n",
    "    use_pretrained_model=1,\n",
    "    num_classes=1,\n",
    "    mini_batch_size=12,  \n",
    "    epochs=30,  \n",
    "    learning_rate=8.690212861278214e-05, \n",
    "    lr_scheduler_step=10,\n",
    "    lr_scheduler_factor=0.1,\n",
    "    optimizer='adam',  \n",
    "    momentum=0.9937708941341203,  \n",
    "    weight_decay=0.7641299369416104, \n",
    "    overlap_threshold=0.5,\n",
    "    nms_threshold=0.45,\n",
    "    image_shape=512,\n",
    "    label_width=8,\n",
    "    num_training_samples=887\n",
    ")\n",
    "\n",
    "train_input = sagemaker.inputs.TrainingInput(s3_data=f's3://{default_bucket}/train/', content_type='application/x-recordio')\n",
    "val_input = sagemaker.inputs.TrainingInput(s3_data=f's3://{default_bucket}/valid/', content_type='application/x-recordio')\n",
    "\n",
    "od_model.fit({'train': train_input, 'validation': val_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "model_package_group_name = \"SSD\" + str(round(time.time()))\n",
    "model_package_group_input_dict = {\n",
    " \"ModelPackageGroupName\" : model_package_group_name,\n",
    " \"ModelPackageGroupDescription\" : \"Sample model package group\"\n",
    "}\n",
    "\n",
    "create_model_package_group_response = sm_client.create_model_package_group(**model_package_group_input_dict)\n",
    "print('ModelPackageGroup Arn : {}'.format(create_model_package_group_response['ModelPackageGroupArn']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_url = \"s3://your-bucket-name/model.tar.gz\"\n",
    "\n",
    "modelpackage_inference_specification =  {\n",
    "    \"InferenceSpecification\": {\n",
    "      \"Containers\": [\n",
    "         {\n",
    "            \"Image\": image_uri,\n",
    "\t    \"ModelDataUrl\": model_url\n",
    "         }\n",
    "      ],\n",
    "      \"SupportedContentTypes\": [ \"text/csv\" ],\n",
    "      \"SupportedResponseMIMETypes\": [ \"text/csv\" ],\n",
    "   }\n",
    " }\n",
    "\n",
    "# Alternatively, you can specify the model source like this:\n",
    "# modelpackage_inference_specification[\"InferenceSpecification\"][\"Containers\"][0][\"ModelDataUrl\"]=model_url\n",
    "\n",
    "create_model_package_input_dict = {\n",
    "    \"ModelPackageGroupName\" : model_package_group_name,\n",
    "    \"ModelPackageDescription\" : \"SSD\",\n",
    "    \"ModelApprovalStatus\" : \"PendingManualApproval\"\n",
    "}\n",
    "create_model_package_input_dict.update(modelpackage_inference_specification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_model_package_response = sm_client.create_model_package(**create_model_package_input_dict)\n",
    "model_package_arn = create_model_package_response[\"ModelPackageArn\"]\n",
    "print('ModelPackage Version ARN : {}'.format(model_package_arn))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
